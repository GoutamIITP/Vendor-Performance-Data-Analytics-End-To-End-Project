{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c66c067",
   "metadata": {},
   "source": [
    "# Data Ingestion Notebook\n",
    "\n",
    "This notebook is designed to automate the process of loading raw CSV files into the `inventory.db` SQLite database.  \n",
    "It is modular and ready for future data updates or re-ingestion.\n",
    "\n",
    "**Key Features:**\n",
    "- Handles both small and large CSV files (large files are loaded in chunks for efficiency).\n",
    "- Automatically detects and ingests all CSVs in the `data/` folder.\n",
    "- Creates or replaces tables in the database as needed.\n",
    "- Prints progress and row counts for transparency.\n",
    "\n",
    "**How to Use:**\n",
    "1. Place your raw CSV files in the `data/` directory.\n",
    "2. Run the notebook cells in order.\n",
    "3. The script will load each CSV as a table in `inventory.db`.\n",
    "4. After ingestion, you can preview tables and row counts directly from the notebook.\n",
    "\n",
    "**Why use this notebook?**\n",
    "- Ensures your database is always up-to-date with the latest raw data.\n",
    "- Makes it easy to re-run ingestion for new data drops or corrections.\n",
    "- Keeps your data pipeline reproducible and transparent.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c7f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed69d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the database engine\n",
    "engine = create_engine('sqlite:///inventory.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7cd80be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "begin_inventory.csv\n",
      "end_inventory.csv\n",
      "purchases.csv\n",
      "purchase_prices.csv\n",
      "sales.csv\n",
      "vendor_invoice.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data'):\n",
    " \n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07f2e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ingest a DataFrame into the database\n",
    "def ingest_db(df, table_name, engine):\n",
    "    df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "    print(f\"Uploaded: {table_name} | Shape: {df.shape}\")\n",
    "\n",
    "# Function to ingest large CSVs in chunks\n",
    "def ingest_large_csv(file_path, table_name, engine, chunk_size=100_000):\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        mode = 'replace' if i == 0 else 'append'\n",
    "        chunk.to_sql(table_name, con=engine, if_exists=mode, index=False)\n",
    "        print(f\"{table_name} - Chunk {i+1} inserted: {chunk.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca2dc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to load all CSVs from the data folder\n",
    "def load_raw_data():\n",
    "    start = time.time()\n",
    "    data_folder = 'data'\n",
    "    heavy = {'vendor_invoice.csv', 'sales.csv', 'purchases.csv'}  # Large files to chunk\n",
    "\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            path = os.path.join(data_folder, file)\n",
    "            table = file[:-4]\n",
    "            if file in heavy:\n",
    "                print(f\"Chunk loading heavy file: {file}\")\n",
    "                ingest_large_csv(path, table, engine)\n",
    "            else:\n",
    "                df = pd.read_csv(path)\n",
    "                ingest_db(df, table, engine)\n",
    "\n",
    "    dt = (time.time() - start) / 60\n",
    "    print(f\"\\nIngestion done in {dt:.2f} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32762eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: begin_inventory | Shape: (206529, 9)\n",
      "Uploaded: end_inventory | Shape: (224489, 9)\n",
      "Chunk loading heavy file: purchases.csv\n",
      "purchases - Chunk 1 inserted: (100000, 16)\n",
      "purchases - Chunk 2 inserted: (100000, 16)\n",
      "purchases - Chunk 3 inserted: (100000, 16)\n",
      "purchases - Chunk 4 inserted: (100000, 16)\n",
      "purchases - Chunk 5 inserted: (100000, 16)\n",
      "purchases - Chunk 6 inserted: (100000, 16)\n",
      "purchases - Chunk 7 inserted: (100000, 16)\n",
      "purchases - Chunk 8 inserted: (100000, 16)\n",
      "purchases - Chunk 9 inserted: (100000, 16)\n",
      "purchases - Chunk 10 inserted: (100000, 16)\n",
      "purchases - Chunk 11 inserted: (100000, 16)\n",
      "purchases - Chunk 12 inserted: (100000, 16)\n",
      "purchases - Chunk 13 inserted: (100000, 16)\n",
      "purchases - Chunk 14 inserted: (100000, 16)\n",
      "purchases - Chunk 15 inserted: (100000, 16)\n",
      "purchases - Chunk 16 inserted: (100000, 16)\n",
      "purchases - Chunk 17 inserted: (100000, 16)\n",
      "purchases - Chunk 18 inserted: (100000, 16)\n",
      "purchases - Chunk 19 inserted: (100000, 16)\n",
      "purchases - Chunk 20 inserted: (100000, 16)\n",
      "purchases - Chunk 21 inserted: (100000, 16)\n",
      "purchases - Chunk 22 inserted: (100000, 16)\n",
      "purchases - Chunk 23 inserted: (100000, 16)\n",
      "purchases - Chunk 24 inserted: (72474, 16)\n",
      "Uploaded: purchase_prices | Shape: (12261, 9)\n",
      "Chunk loading heavy file: sales.csv\n",
      "sales - Chunk 1 inserted: (100000, 14)\n",
      "sales - Chunk 2 inserted: (100000, 14)\n",
      "sales - Chunk 3 inserted: (100000, 14)\n",
      "sales - Chunk 4 inserted: (100000, 14)\n",
      "sales - Chunk 5 inserted: (100000, 14)\n",
      "sales - Chunk 6 inserted: (100000, 14)\n",
      "sales - Chunk 7 inserted: (100000, 14)\n",
      "sales - Chunk 8 inserted: (100000, 14)\n",
      "sales - Chunk 9 inserted: (100000, 14)\n",
      "sales - Chunk 10 inserted: (100000, 14)\n",
      "sales - Chunk 11 inserted: (100000, 14)\n",
      "sales - Chunk 12 inserted: (100000, 14)\n",
      "sales - Chunk 13 inserted: (100000, 14)\n",
      "sales - Chunk 14 inserted: (100000, 14)\n",
      "sales - Chunk 15 inserted: (100000, 14)\n",
      "sales - Chunk 16 inserted: (100000, 14)\n",
      "sales - Chunk 17 inserted: (100000, 14)\n",
      "sales - Chunk 18 inserted: (100000, 14)\n",
      "sales - Chunk 19 inserted: (100000, 14)\n",
      "sales - Chunk 20 inserted: (100000, 14)\n",
      "sales - Chunk 21 inserted: (100000, 14)\n",
      "sales - Chunk 22 inserted: (100000, 14)\n",
      "sales - Chunk 23 inserted: (100000, 14)\n",
      "sales - Chunk 24 inserted: (100000, 14)\n",
      "sales - Chunk 25 inserted: (100000, 14)\n",
      "sales - Chunk 26 inserted: (100000, 14)\n",
      "sales - Chunk 27 inserted: (100000, 14)\n",
      "sales - Chunk 28 inserted: (100000, 14)\n",
      "sales - Chunk 29 inserted: (100000, 14)\n",
      "sales - Chunk 30 inserted: (100000, 14)\n",
      "sales - Chunk 31 inserted: (100000, 14)\n",
      "sales - Chunk 32 inserted: (100000, 14)\n",
      "sales - Chunk 33 inserted: (100000, 14)\n",
      "sales - Chunk 34 inserted: (100000, 14)\n",
      "sales - Chunk 35 inserted: (100000, 14)\n",
      "sales - Chunk 36 inserted: (100000, 14)\n",
      "sales - Chunk 37 inserted: (100000, 14)\n",
      "sales - Chunk 38 inserted: (100000, 14)\n",
      "sales - Chunk 39 inserted: (100000, 14)\n",
      "sales - Chunk 40 inserted: (100000, 14)\n",
      "sales - Chunk 41 inserted: (100000, 14)\n",
      "sales - Chunk 42 inserted: (100000, 14)\n",
      "sales - Chunk 43 inserted: (100000, 14)\n",
      "sales - Chunk 44 inserted: (100000, 14)\n",
      "sales - Chunk 45 inserted: (100000, 14)\n",
      "sales - Chunk 46 inserted: (100000, 14)\n",
      "sales - Chunk 47 inserted: (100000, 14)\n",
      "sales - Chunk 48 inserted: (100000, 14)\n",
      "sales - Chunk 49 inserted: (100000, 14)\n",
      "sales - Chunk 50 inserted: (100000, 14)\n",
      "sales - Chunk 51 inserted: (100000, 14)\n",
      "sales - Chunk 52 inserted: (100000, 14)\n",
      "sales - Chunk 53 inserted: (100000, 14)\n",
      "sales - Chunk 54 inserted: (100000, 14)\n",
      "sales - Chunk 55 inserted: (100000, 14)\n",
      "sales - Chunk 56 inserted: (100000, 14)\n",
      "sales - Chunk 57 inserted: (100000, 14)\n",
      "sales - Chunk 58 inserted: (100000, 14)\n",
      "sales - Chunk 59 inserted: (100000, 14)\n",
      "sales - Chunk 60 inserted: (100000, 14)\n",
      "sales - Chunk 61 inserted: (100000, 14)\n",
      "sales - Chunk 62 inserted: (100000, 14)\n",
      "sales - Chunk 63 inserted: (100000, 14)\n",
      "sales - Chunk 64 inserted: (100000, 14)\n",
      "sales - Chunk 65 inserted: (100000, 14)\n",
      "sales - Chunk 66 inserted: (100000, 14)\n",
      "sales - Chunk 67 inserted: (100000, 14)\n",
      "sales - Chunk 68 inserted: (100000, 14)\n",
      "sales - Chunk 69 inserted: (100000, 14)\n",
      "sales - Chunk 70 inserted: (100000, 14)\n",
      "sales - Chunk 71 inserted: (100000, 14)\n",
      "sales - Chunk 72 inserted: (100000, 14)\n",
      "sales - Chunk 73 inserted: (100000, 14)\n",
      "sales - Chunk 74 inserted: (100000, 14)\n",
      "sales - Chunk 75 inserted: (100000, 14)\n",
      "sales - Chunk 76 inserted: (100000, 14)\n",
      "sales - Chunk 77 inserted: (100000, 14)\n",
      "sales - Chunk 78 inserted: (100000, 14)\n",
      "sales - Chunk 79 inserted: (100000, 14)\n",
      "sales - Chunk 80 inserted: (100000, 14)\n",
      "sales - Chunk 81 inserted: (100000, 14)\n",
      "sales - Chunk 82 inserted: (100000, 14)\n",
      "sales - Chunk 83 inserted: (100000, 14)\n",
      "sales - Chunk 84 inserted: (100000, 14)\n",
      "sales - Chunk 85 inserted: (100000, 14)\n",
      "sales - Chunk 86 inserted: (100000, 14)\n",
      "sales - Chunk 87 inserted: (100000, 14)\n",
      "sales - Chunk 88 inserted: (100000, 14)\n",
      "sales - Chunk 89 inserted: (100000, 14)\n",
      "sales - Chunk 90 inserted: (100000, 14)\n",
      "sales - Chunk 91 inserted: (100000, 14)\n",
      "sales - Chunk 92 inserted: (100000, 14)\n",
      "sales - Chunk 93 inserted: (100000, 14)\n",
      "sales - Chunk 94 inserted: (100000, 14)\n",
      "sales - Chunk 95 inserted: (100000, 14)\n",
      "sales - Chunk 96 inserted: (100000, 14)\n",
      "sales - Chunk 97 inserted: (100000, 14)\n",
      "sales - Chunk 98 inserted: (100000, 14)\n",
      "sales - Chunk 99 inserted: (100000, 14)\n",
      "sales - Chunk 100 inserted: (100000, 14)\n",
      "sales - Chunk 101 inserted: (100000, 14)\n",
      "sales - Chunk 102 inserted: (100000, 14)\n",
      "sales - Chunk 103 inserted: (100000, 14)\n",
      "sales - Chunk 104 inserted: (100000, 14)\n",
      "sales - Chunk 105 inserted: (100000, 14)\n",
      "sales - Chunk 106 inserted: (100000, 14)\n",
      "sales - Chunk 107 inserted: (100000, 14)\n",
      "sales - Chunk 108 inserted: (100000, 14)\n",
      "sales - Chunk 109 inserted: (100000, 14)\n",
      "sales - Chunk 110 inserted: (100000, 14)\n",
      "sales - Chunk 111 inserted: (100000, 14)\n",
      "sales - Chunk 112 inserted: (100000, 14)\n",
      "sales - Chunk 113 inserted: (100000, 14)\n",
      "sales - Chunk 114 inserted: (100000, 14)\n",
      "sales - Chunk 115 inserted: (100000, 14)\n",
      "sales - Chunk 116 inserted: (100000, 14)\n",
      "sales - Chunk 117 inserted: (100000, 14)\n",
      "sales - Chunk 118 inserted: (100000, 14)\n",
      "sales - Chunk 119 inserted: (100000, 14)\n",
      "sales - Chunk 120 inserted: (100000, 14)\n",
      "sales - Chunk 121 inserted: (100000, 14)\n",
      "sales - Chunk 122 inserted: (100000, 14)\n",
      "sales - Chunk 123 inserted: (100000, 14)\n",
      "sales - Chunk 124 inserted: (100000, 14)\n",
      "sales - Chunk 125 inserted: (100000, 14)\n",
      "sales - Chunk 126 inserted: (100000, 14)\n",
      "sales - Chunk 127 inserted: (100000, 14)\n",
      "sales - Chunk 128 inserted: (100000, 14)\n",
      "sales - Chunk 129 inserted: (25363, 14)\n",
      "Chunk loading heavy file: vendor_invoice.csv\n",
      "vendor_invoice - Chunk 1 inserted: (5543, 10)\n",
      "\n",
      "Ingestion done in 2.99 mins\n"
     ]
    }
   ],
   "source": [
    "# # Run the ingestion process\n",
    "\n",
    "load_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d76f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables found in inventory.db:\n",
      "['begin_inventory', 'end_inventory', 'purchase_prices', 'purchases', 'sales', 'vendor_invoice']\n"
     ]
    }
   ],
   "source": [
    "# Check what tables are now in the database\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "print(\"Tables found in inventory.db:\")\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "459bf9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           InventoryId  Store  Brand                 Description        Size  \\\n",
      "0  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "1  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "2  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "3  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "4  1_HARDERSFIELD_1005      1   1005     Maker's Mark Combo Pack  375mL 2 Pk   \n",
      "\n",
      "   SalesQuantity  SalesDollars  SalesPrice   SalesDate  Volume  \\\n",
      "0              1         16.49       16.49  2024-01-01   750.0   \n",
      "1              2         32.98       16.49  2024-01-02   750.0   \n",
      "2              1         16.49       16.49  2024-01-03   750.0   \n",
      "3              1         14.49       14.49  2024-01-08   750.0   \n",
      "4              2         69.98       34.99  2024-01-09   375.0   \n",
      "\n",
      "   Classification  ExciseTax  VendorNo                   VendorName  \n",
      "0               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "1               1       1.57     12546  JIM BEAM BRANDS COMPANY      \n",
      "2               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "3               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "4               1       0.79     12546  JIM BEAM BRANDS COMPANY      \n"
     ]
    }
   ],
   "source": [
    "# Preview a few rows from a table (e.g., sales)\n",
    "df = pd.read_sql(\"SELECT * FROM sales LIMIT 5\", con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "249f66a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin_inventory: 206529 rows\n",
      "end_inventory: 224489 rows\n",
      "purchase_prices: 12261 rows\n",
      "purchases: 2372474 rows\n",
      "sales: 12825363 rows\n",
      "vendor_invoice: 5543 rows\n"
     ]
    }
   ],
   "source": [
    "# Print row counts for each table\n",
    "for table in inspector.get_table_names():\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) AS rows FROM {table}\", con=engine)\n",
    "    print(f\"{table}: {count['rows'][0]} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
